{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2f9d38-af70-4a24-9cd9-c2bb2fa98302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project - Databricks-Skew -BroadcastJoin-Solutions\n",
    "\n",
    "## Overview\n",
    "This project demonstrates the use of the broadcast join technique to mitigate data skew in Apache Spark join operations. We will join the Online Retail Dataset with a small lookup table, optimize performance using broadcasting, and document the results. This builds on the salting technique from Project 1 by addressing skew in a join context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bccb69b-1d7a-488a-9f64-a8762c26fb2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset\n",
    "- **Source**: [Online Retail Dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail)\n",
    "- **Details**: 541,909 records, 44.5 MB, stored at `/FileStore/tables/online_retail.csv`.\n",
    "- **Lookup Table**: A manually created `customer_regions` DataFrame mapping `CustomerID` to `Region`.\n",
    "\n",
    "## Environment Setup\n",
    "- **Platform**: Databricks Community Edition\n",
    "- **Cluster**: 1 driver, 1 worker\n",
    "- **Notebook Path**: `/Users/uday91@gmail.com/Project - Databricks-Skew -BroadcastJoin-Solutions`\n",
    "- **Objective**: Configure the environment and load the dataset for join optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f658b6e-591a-4b4c-9c14-7468045ab34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d26e924-4b69-468f-92b5-3c886bb5047b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session (automatically managed by Databricks)\n",
    "spark = SparkSession.builder.appName(\"Broadcast Join Example\").getOrCreate()\n",
    "\n",
    "# Load the online retail dataset\n",
    "retail_df = spark.read.csv(\"/FileStore/tables/online_retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Verify loading\n",
    "print(\"Retail DataFrame loaded successfully.\")\n",
    "\n",
    "retail_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41313f5c-c61d-437c-b678-fbabe780d692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a small lookup table for customer regions\n",
    "customer_regions = spark.createDataFrame([\n",
    "    (17841, \"Europe\"),\n",
    "    (14911, \"North America\"),\n",
    "    (14096, \"Asia\")\n",
    "], [\"CustomerID\", \"Region\"])\n",
    "\n",
    "# Verify lookup table\n",
    "print(\"Customer Regions Lookup Table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fafdcb6-8c3c-4ba9-91b4-e040c6823b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrames to verify\n",
    "print(\"Retail DataFrame (first 5 rows):\")\n",
    "retail_df.show(5)\n",
    "print(\"Customer Regions Lookup Table:\")\n",
    "customer_regions.show()\n",
    "\n",
    "# Check schema for reference\n",
    "print(\"Retail DataFrame Schema:\")\n",
    "retail_df.printSchema()\n",
    "print(\"Customer Regions Schema:\")\n",
    "customer_regions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb07c3a-d14e-4a47-9701-f93cd82a7862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Perform Initial Join\n",
    "- **Action**: Perform a normal shuffle join between `retail_df` and `customer_regions` on `CustomerID` to establish a baseline.\n",
    "- **Outcome**: Verify the join results and observe potential skew in Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4649208-f686-4779-8dff-87db05ab6ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform a normal shuffle join\n",
    "joined_df = retail_df.join(customer_regions, \"CustomerID\", \"left_outer\")\n",
    "\n",
    "# Display the first 5 rows of the joined DataFrame\n",
    "print(\"Joined DataFrame (first 5 rows):\")\n",
    "joined_df.show(5)\n",
    "\n",
    "# Check the number of rows to confirm the join\n",
    "print(f\"Total rows in joined DataFrame: {joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6676ce-9997-49ad-af55-33ba637e94a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.1: Skew Analysis from Shuffle Join\n",
    "- **Action**: Analyzed Spark UI for the shuffle join in Cell 8 (Stage 24, Job 16).\n",
    "- **Observations**: Task durations 0.2-0.5 s (average ~0.375 s), Shuffle Read 19.4-43.4 KiB, no significant skew detected. Possible implicit broadcast of `customer_regions` due to its small size (3 rows).\n",
    "- **Outcome**: Established baseline performance for comparison with explicit broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c474b38d-bc4b-4dda-a35c-11473cbd78c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform a broadcast join\n",
    "broadcast_joined_df = retail_df.join(broadcast(customer_regions), \"CustomerID\", \"left_outer\")\n",
    "\n",
    "# Display the first 5 rows of the broadcast-joined DataFrame\n",
    "print(\"Broadcast Joined DataFrame (first 5 rows):\")\n",
    "broadcast_joined_df.show(5)\n",
    "\n",
    "# Check the number of rows to confirm the join\n",
    "print(f\"Total rows in broadcast-joined DataFrame: {broadcast_joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec337e4-3c38-4f3c-8651-61af6f44f3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.2: Isolate Null CustomerID for Skew Analysis\n",
    "- **Action**: Filter the dataset to rows with null CustomerID to force skew analysis in a shuffle join.\n",
    "- **Outcome**: Verify skew impact and prepare for broadcast join optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8100311-d4cf-4e3d-99e4-28619082ea16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter for null CustomerID\n",
    "null_retail_df = retail_df.filter(\"CustomerID is null\")\n",
    "\n",
    "# Perform a shuffle join on the null subset\n",
    "null_joined_df = null_retail_df.join(customer_regions, \"CustomerID\", \"left_outer\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"Joined DataFrame for Null CustomerID (first 5 rows):\")\n",
    "null_joined_df.show(5)\n",
    "\n",
    "# Check the number of rows\n",
    "print(f\"Total rows in null-joined DataFrame: {null_joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1439513c-3f2c-4a5d-9eee-254a30e76d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.3: Skew Analysis for Null Subset\n",
    "- **Action**: Analyze Spark UI for the shuffle join on null CustomerID data.\n",
    "- **Outcome**: Document skew evidence for comparison with broadcast join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85732412-8663-47b9-be6f-782f03255b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: Check Spark UI manually for task durations and shuffle sizes\n",
    "print(\"Check Spark UI (Stages tab) for shuffle join on null CustomerID data.\")\n",
    "print(f\"Total rows in null-joined DataFrame: {null_joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24e22998-e5a4-4d78-ba80-ae059718dd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Comparison: Shuffle Join vs. Broadcast Join\n",
    "\n",
    "| Metric            | Shuffle Join (Job ID 34) | Broadcast Join (Job ID 39) |\n",
    "|-------------------|--------------------------|----------------------------|\n",
    "| Duration          | 0.2 s                    | 37 ms                      |\n",
    "| Tasks             | 1                        | 1                          |\n",
    "| Data Skew         | None observed            | None observed              |\n",
    "\n",
    "**Notes**: The shuffle join (Job ID 34) executed efficiently with a single task, likely due to Spark's auto-broadcast optimization given the small lookup table size (3 rows). The broadcast join (Job ID 39) further confirms this optimization, completing even faster. No significant data skew was observed in either case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2890cf68-4d6f-4d18-bc87-1b49be02580e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Optimize with Broadcast Join\n",
    "- **Action**: Use the `broadcast` function to optimize the join on null CustomerID data.\n",
    "- **Outcome**: Compare performance with the shuffle join and verify data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19cf525-f807-4ce6-9a64-3b08c8535e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform a broadcast join on the null subset\n",
    "broadcast_null_joined_df = null_retail_df.join(broadcast(customer_regions), \"CustomerID\", \"left_outer\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"Broadcast Joined DataFrame for Null CustomerID (first 5 rows):\")\n",
    "broadcast_null_joined_df.show(5)\n",
    "\n",
    "# Check the number of rows\n",
    "print(f\"Total rows in broadcast-null-joined DataFrame: {broadcast_null_joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2022cf28-b649-440a-9eef-23697a26918a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3.1: Broadcast Join Performance Analysis\n",
    "- **Action**: Analyzed Spark UI for the broadcast join on null CustomerID data.\n",
    "- **Outcome**: Document performance improvement compared to shuffle join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2417c7-4965-414b-b3a2-49d5865ea810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Comparison: Shuffle Join vs. Broadcast Join\n",
    "\n",
    "| Metric            | Shuffle Join (Job ID 34) | Broadcast Join (Job ID 39) |\n",
    "|-------------------|--------------------------|----------------------------|\n",
    "| Duration          | 0.2 s                    | 37 ms                      |\n",
    "| Tasks             | 1                        | 1                          |\n",
    "| Data Skew         | None observed            | None observed              |\n",
    "\n",
    "**Notes**: The shuffle join (Job ID 34) executed efficiently with a single task, likely due to Spark's auto-broadcast optimization given the small lookup table size (3 rows). The broadcast join (Job ID 39) further confirms this optimization, completing even faster. No significant data skew was observed in either case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2ed9ba-12db-424f-811c-8a6aa511f513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: Update with Spark UI findings\n",
    "print(\"Check Spark UI (Stages tab) for broadcast join on null CustomerID data.\")\n",
    "print(f\"Total rows in broadcast-null-joined DataFrame: {broadcast_null_joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847c846b-0ae4-48a0-94dc-ae8c8ebf7ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This project analyzed the effectiveness of broadcast joins in mitigating data skew compared to shuffle joins, using the Online Retail Dataset and a small customer_regions lookup table. Key findings include:\n",
    "\n",
    "- **Broadcast Join Efficiency**: The broadcast join (e.g., Job ID 39, 37 ms) outperformed the shuffle join (e.g., Job ID 34, 0.2 s) due to the elimination of shuffle overhead, completing with a single task.\n",
    "- **Optimization Impact**: Spark's auto-broadcasting optimized the shuffle join into a broadcast-like execution, as evidenced by the single-task performance and skipped stages in Job ID 34, likely triggered by the 3-row lookup table being below the default 10MB threshold.\n",
    "- **Data Skew**: No significant data skew was observed in either join type, attributed to the small dataset size and Spark's optimization. Manual skew analysis was not required due to this optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d012f3-3f6c-42c3-8c8e-01a1642a17ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9dfef2f-75a9-4f8a-9b5a-562b6590399a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1e26ba-f15d-4803-b8f3-8fe726c60d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc946882-332c-453a-95a6-9c3186b59d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b2a998-2895-4d74-a9b7-ced20066fb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac638065-5925-44e7-8f7c-6de35ebcc51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project - Databricks-Skew -BroadcastJoin-Solutions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
